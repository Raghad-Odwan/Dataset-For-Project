{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mOj0_ntHfRUC",
        "AXsSX7RlgGEG",
        "B85Q5D26hwSB"
      ],
      "authorship_tag": "ABX9TyOPZSFD1Kuhjk6UOuFDMFlQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raghad-Odwan/Dataset-For-Project/blob/main/Dataset_DermAI_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DermAI** â€” Dataset Cleaning & Preprocessing"
      ],
      "metadata": {
        "id": "TTL3vULbebk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Uploading the dataset from Google Drive and extracted Dataset\n"
      ],
      "metadata": {
        "id": "mOj0_ntHfRUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18I3MTqnfVsF",
        "outputId": "f3065855-d25f-4705-9fa3-02ec3ac3461a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extracted Dataset\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/Dataset.zip\"\n",
        "extract_path = \"/content/Dataset\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Dataset extracted successfully!\")\n",
        "os.listdir(extract_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzyRO0GJfndq",
        "outputId": "121fd288-c862-4ecb-d803-b56d1294bcfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted successfully!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dataset']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/Dataset/Dataset\"\n",
        "\n",
        "for item in os.listdir(dataset_path):\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68fO5qBbfzna",
        "outputId": "373abade-ffd8-42eb-d7bd-e0b30cccf3f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "malignant\n",
            "benign\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialize the environment and import required libraries"
      ],
      "metadata": {
        "id": "AXsSX7RlgGEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade \"tqdm>=4.67\" \"opencv-python-headless>=4.9.0.80\" \"albumentations>=1.3.1\""
      ],
      "metadata": {
        "id": "e64dsnsPgPJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import shutil\n",
        "import hashlib\n",
        "import random\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "\n",
        "# Ignore warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "9A7VDMtOgXDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Cleaning and Exploration**"
      ],
      "metadata": {
        "id": "k3s4vKmggwIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/Dataset/Dataset\"\n",
        "folders = [\"benign\", \"malignant\"]\n"
      ],
      "metadata": {
        "id": "PSku2Faahm_A"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a folder for corrupted or duplicate images\n",
        "dup_dir = os.path.join(base_dir, \"duplicates_or_corrupted\")\n",
        "os.makedirs(dup_dir, exist_ok=True)\n",
        "\n",
        "print(\"Base directory:\", base_dir)\n",
        "print(\"Duplicate/Corrupted folder created at:\", dup_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce8xJWmPhrAF",
        "outputId": "04466f6c-ae26-4a06-f7d2-38cedfa5dc76"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base directory: /content/Dataset/Dataset\n",
            "Duplicate/Corrupted folder created at: /content/Dataset/Dataset/duplicates_or_corrupted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Data Cleaning and Quality Verification"
      ],
      "metadata": {
        "id": "B85Q5D26hwSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCleaner:\n",
        "    def __init__(self, base_path):\n",
        "        self.base_path = Path(base_path)\n",
        "        self.folders_to_check = ['benign', 'malignant']\n",
        "\n",
        "        # Create folder for problematic files\n",
        "        self.problem_folder = self.base_path / 'duplicates_or_corrupted'\n",
        "        self.problem_folder.mkdir(exist_ok=True)\n",
        "\n",
        "        # Statistics\n",
        "        self.stats = {\n",
        "            'total_checked': 0,\n",
        "            'corrupted': 0,\n",
        "            'duplicates': 0,\n",
        "            'low_quality': 0,\n",
        "            'healthy': 0\n",
        "        }\n",
        "\n",
        "        # Store image hashes\n",
        "        self.image_hashes = defaultdict(list)\n",
        "\n",
        "    def calculate_hash(self, image_path):\n",
        "        try:\n",
        "            hasher = hashlib.md5()\n",
        "            with open(image_path, 'rb') as f:\n",
        "                buf = f.read()\n",
        "                hasher.update(buf)\n",
        "            return hasher.hexdigest()\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating hash: {image_path} - {e}\")\n",
        "            return None\n",
        "\n",
        "    def is_image_corrupted(self, image_path):\n",
        "        try:\n",
        "            with Image.open(image_path) as img:\n",
        "                img.verify()\n",
        "\n",
        "            with Image.open(image_path) as img:\n",
        "                img.load()\n",
        "\n",
        "            return False\n",
        "        except Exception:\n",
        "            return True\n",
        "\n",
        "    def check_image_quality(self, image_path, min_width=50, min_height=50):\n",
        "        try:\n",
        "            with Image.open(image_path) as img:\n",
        "                width, height = img.size\n",
        "\n",
        "                if width < min_width or height < min_height:\n",
        "                    return False\n",
        "\n",
        "                file_size = os.path.getsize(image_path)\n",
        "                if file_size < 1000:\n",
        "                    return False\n",
        "\n",
        "            return True\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "    def move_to_problem_folder(self, image_path):\n",
        "        try:\n",
        "            source_folder = image_path.parent.name\n",
        "            dest_subfolder = self.problem_folder / source_folder\n",
        "            dest_subfolder.mkdir(exist_ok=True)\n",
        "\n",
        "            destination = dest_subfolder / image_path.name\n",
        "\n",
        "            if destination.exists():\n",
        "                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "                destination = dest_subfolder / f\"{image_path.stem}_{timestamp}{image_path.suffix}\"\n",
        "\n",
        "            shutil.move(str(image_path), str(destination))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error moving file: {image_path.name} - {e}\")\n",
        "\n",
        "    def clean_folder(self, folder_name):\n",
        "        folder_path = self.base_path / folder_name\n",
        "\n",
        "        if not folder_path.exists():\n",
        "            print(f\"Folder not found: {folder_name}\")\n",
        "            return\n",
        "\n",
        "        print(f\"Processing: {folder_name}...\", end=' ')\n",
        "\n",
        "        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif']\n",
        "        images = [f for f in folder_path.iterdir()\n",
        "                 if f.is_file() and f.suffix.lower() in image_extensions]\n",
        "\n",
        "        for img_path in images:\n",
        "            self.stats['total_checked'] += 1\n",
        "\n",
        "            # Check corruption\n",
        "            if self.is_image_corrupted(img_path):\n",
        "                self.move_to_problem_folder(img_path)\n",
        "                self.stats['corrupted'] += 1\n",
        "                continue\n",
        "\n",
        "            # Check quality\n",
        "            if not self.check_image_quality(img_path):\n",
        "                self.move_to_problem_folder(img_path)\n",
        "                self.stats['low_quality'] += 1\n",
        "                continue\n",
        "\n",
        "            # Check duplicates\n",
        "            img_hash = self.calculate_hash(img_path)\n",
        "            if img_hash:\n",
        "                if img_hash in self.image_hashes:\n",
        "                    self.move_to_problem_folder(img_path)\n",
        "                    self.stats['duplicates'] += 1\n",
        "                else:\n",
        "                    self.image_hashes[img_hash].append(str(img_path))\n",
        "                    self.stats['healthy'] += 1\n",
        "\n",
        "        print(f\"Done ({len(images)} images checked)\")\n",
        "\n",
        "    def clean_all(self):\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"Starting cleaning process\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        for folder in self.folders_to_check:\n",
        "            self.clean_folder(folder)\n",
        "\n",
        "        self.print_summary()\n",
        "\n",
        "    def print_summary(self):\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"Cleaning Summary\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Total checked: {self.stats['total_checked']}\")\n",
        "        print(f\"Healthy images: {self.stats['healthy']}\")\n",
        "        print(f\"Corrupted: {self.stats['corrupted']}\")\n",
        "        print(f\"Duplicates: {self.stats['duplicates']}\")\n",
        "        print(f\"Low quality: {self.stats['low_quality']}\")\n",
        "        print(f\"\\nProblematic files moved to: {self.problem_folder}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "\n",
        "# For Google Colab - use the path you already created\n",
        "base_path = \"/content/Dataset/Dataset\"\n",
        "\n",
        "# Check if path exists\n",
        "if not os.path.exists(base_path):\n",
        "    print(f\"Error: Path does not exist: {base_path}\")\n",
        "    print(\"Current working directory:\", os.getcwd())\n",
        "    print(\"\\nAvailable folders:\")\n",
        "    if os.path.exists(\"/content/Dataset\"):\n",
        "        print(os.listdir(\"/content/Dataset\"))\n",
        "else:\n",
        "    print(f\"Base path found: {base_path}\")\n",
        "    print(f\"Contents: {os.listdir(base_path)}\")\n",
        "\n",
        "    cleaner = ImageCleaner(base_path)\n",
        "    cleaner.clean_all()\n",
        "    print(\"\\nCleaning completed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV-b53TpigGg",
        "outputId": "fd2275a4-3d07-486a-f36f-b3761f090a54"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base path found: /content/Dataset/Dataset\n",
            "Contents: ['duplicates_or_corrupted', 'malignant', 'benign']\n",
            "\n",
            "============================================================\n",
            "Starting cleaning process\n",
            "============================================================\n",
            "Processing: benign... Done (13294 images checked)\n",
            "Processing: malignant... Done (6211 images checked)\n",
            "\n",
            "============================================================\n",
            "Cleaning Summary\n",
            "============================================================\n",
            "Total checked: 19505\n",
            "Healthy images: 19505\n",
            "Corrupted: 0\n",
            "Duplicates: 0\n",
            "Low quality: 0\n",
            "\n",
            "Problematic files moved to: /content/Dataset/Dataset/duplicates_or_corrupted\n",
            "============================================================\n",
            "\n",
            "Cleaning completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Data preprocessing (resizing, split)"
      ],
      "metadata": {
        "id": "0jiDJ3Opg7GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "8gyqWyojivsQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = \"/content/Dataset/Dataset\"\n",
        "FOLDERS = [\"benign\", \"malignant\"]\n",
        "IMG_SIZE = (224, 224)\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "TRAIN_RATIO = 0.7\n",
        "VAL_RATIO = 0.15\n",
        "TEST_RATIO = 0.15\n",
        "SPLIT_ROOT = os.path.join(BASE_DIR)"
      ],
      "metadata": {
        "id": "mFjjUZJMiyCV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize all images to 224x224 and overwrite the originals as JPG files\n",
        "for cat in FOLDERS:\n",
        "    src_dir = os.path.join(BASE_DIR, cat)\n",
        "    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n",
        "\n",
        "    for fname in tqdm(files, desc=f\"Resizing {cat}\"):\n",
        "        src_path = os.path.join(src_dir, fname)\n",
        "        try:\n",
        "            if not fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                continue\n",
        "\n",
        "\n",
        "            # read image\n",
        "            img = cv2.imread(src_path)\n",
        "            if img is None:\n",
        "                print(f\" Skipped unreadable file: {fname}\")\n",
        "                continue\n",
        "\n",
        "            # resize image\n",
        "            resized = cv2.resize(img, IMG_SIZE, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "            # save resized image as JPG (overwrite or replace)\n",
        "            base_name = os.path.splitext(fname)[0]\n",
        "            save_name = base_name + \".jpg\"\n",
        "            save_path = os.path.join(src_dir, save_name)\n",
        "            cv2.imwrite(save_path, resized)\n",
        "\n",
        "            # remove original file if extension changed\n",
        "            if save_path != src_path:\n",
        "                os.remove(src_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Skipped {fname} due to error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snHfRfBRi_Lv",
        "outputId": "93fefb1b-9470-43a7-ed1b-7774f3b2ab25"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resizing benign: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13294/13294 [00:08<00:00, 1562.31it/s]\n",
            "Resizing malignant: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6211/6211 [00:04<00:00, 1540.25it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split dataset: 70% train, 15% val, 15% test**\n"
      ],
      "metadata": {
        "id": "cVoo7-3ppmTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "MX7YhBz3qT8F"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/Dataset/Dataset\"\n",
        "folders = [\"benign\", \"malignant\"]\n",
        "\n",
        "split_dir = \"/content/Dataset_split\"\n",
        "os.makedirs(split_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# Collect all image file paths and labels\n",
        "rows = []\n",
        "for label in folders:\n",
        "    path = os.path.join(base_dir, label)\n",
        "    for fname in os.listdir(path):\n",
        "        if fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            rows.append({'path': os.path.join(path, fname), 'label': label})\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "print(\"Total images found:\", len(df))\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# Stratified split: 70% train, 15% val, 15% test\n",
        "train_temp, test = train_test_split(df, test_size=0.15, stratify=df['label'], random_state=42)\n",
        "train, val = train_test_split(train_temp, test_size=0.1765, stratify=train_temp['label'], random_state=42)\n",
        "# 0.1765 of 85% â‰ˆ 15% of total, resulting in 70/15/15 split\n",
        "\n",
        "# Create destination folders for train, val, and test\n",
        "for subset in ['train', 'val', 'test']:\n",
        "    for label in folders:\n",
        "        os.makedirs(os.path.join(split_dir, subset, label), exist_ok=True)\n",
        "\n",
        "# Copy images to their corresponding subset folders\n",
        "def copy_images(df_subset, subset_name):\n",
        "    print(f\"\\nCopying {subset_name} set ({len(df_subset)} images)...\")\n",
        "    for _, row in tqdm(df_subset.iterrows(), total=len(df_subset)):\n",
        "        dest = os.path.join(split_dir, subset_name, row['label'], os.path.basename(row['path']))\n",
        "        shutil.copy2(row['path'], dest)\n",
        "\n",
        "copy_images(train, \"train\")\n",
        "copy_images(val, \"val\")\n",
        "copy_images(test, \"test\")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n Dataset split completed successfully!\")\n",
        "print(f\"Train: {len(train)} images\")\n",
        "print(f\"Val:   {len(val)} images\")\n",
        "print(f\"Test:  {len(test)} images\")\n",
        "\n",
        "print(\"\\nClass distribution per set:\")\n",
        "print(\"Train:\\n\", train['label'].value_counts())\n",
        "print(\"\\nVal:\\n\", val['label'].value_counts())\n",
        "print(\"\\nTest:\\n\", test['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnUZ-HnxqV-h",
        "outputId": "2cc8b18d-64aa-446e-d9e0-860b791c8965"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images found: 19505\n",
            "label\n",
            "benign       13294\n",
            "malignant     6211\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Copying train set (13652 images)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13652/13652 [00:02<00:00, 5027.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Copying val set (2927 images)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2927/2927 [00:00<00:00, 4866.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Copying test set (2926 images)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2926/2926 [00:00<00:00, 4929.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Dataset split completed successfully!\n",
            "Train: 13652 images\n",
            "Val:   2927 images\n",
            "Test:  2926 images\n",
            "\n",
            "Class distribution per set:\n",
            "Train:\n",
            " label\n",
            "benign       9305\n",
            "malignant    4347\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Val:\n",
            " label\n",
            "benign       1995\n",
            "malignant     932\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Test:\n",
            " label\n",
            "benign       1994\n",
            "malignant     932\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}